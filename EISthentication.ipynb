{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîê Authentication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import shap\n",
    "import sys\n",
    "import time\n",
    "import tsfresh\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from utils.const import *\n",
    "from utils.helperFunctions import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"  # Also affect subprocesses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìç Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing IDs\n",
    "ids = ['id1', 'id2', 'id3']\n",
    "# Choose what ID to process\n",
    "id = 'id2'\n",
    "ids_remove = [x for x in ids if x != id]\n",
    "\n",
    "# Filter features and keep only relevant ones\n",
    "filterFeatures = True\n",
    "\n",
    "# Undersampling\n",
    "fairUndersampling = False       # Each class same number\n",
    "targetedUndersampling = True    # Downsample most frequent class\n",
    "customBalance = False           # Downsample by specifying number of samples for each label\n",
    "\n",
    "# If True, perform authentication. If False, perform identification\n",
    "# - Authentication: binary classification\n",
    "# - Identificatiom: multiclass classification\n",
    "authentication = False\n",
    "\n",
    "# Results names and folders\n",
    "if not os.path.exists(RESULTS):\n",
    "    os.mkdir(RESULTS)\n",
    "    os.mkdir(FIGURES)\n",
    "\n",
    "if authentication:\n",
    "    saveBase = id.upper() + '_AUTH'\n",
    "else:\n",
    "    saveBase = id.upper() + '_IDENT'\n",
    "\n",
    "imageFolder = os.path.join(FIGURES, saveBase)\n",
    "if not os.path.exists(imageFolder):\n",
    "    os.mkdir(imageFolder)\n",
    "\n",
    "if not os.path.exists(COMPLEXITY):\n",
    "    os.mkdir(COMPLEXITY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    'AdaBoost',\n",
    "    'Decision Tree',\n",
    "    'Gaussian Naive Bayes',\n",
    "    'Nearest Neighbors',\n",
    "    'Neural Network',\n",
    "    'Quadratic Discriminant Analysis',\n",
    "    'Random Forest',\n",
    "    'Support Vector Machine'\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    AdaBoostClassifier(random_state=SEED),\n",
    "    DecisionTreeClassifier(random_state=SEED),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(),\n",
    "    MLPClassifier(random_state=SEED),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    RandomForestClassifier(random_state=SEED),\n",
    "    SVC(random_state=SEED),\n",
    "]\n",
    "\n",
    "parameters = [\n",
    "    # AdaBoostClassifier\n",
    "    {\n",
    "        'n_estimators': [50, 100, 150, 200]\n",
    "    },\n",
    "    # DecisionTreeClassifier\n",
    "    {\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_depth': np.arange(3, 20)\n",
    "    },\n",
    "    # GaussianNB\n",
    "    {\n",
    "        'var_smoothing': np.logspace(0, -9, num=100)\n",
    "    },\n",
    "    # KNeighborsClassifier\n",
    "    {\n",
    "        'n_neighbors': list(range(1, 20)),\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    # MLPClassifier\n",
    "    {\n",
    "        'hidden_layer_sizes': [(50, ), (100, ), (200, )],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['adam', 'sgd']\n",
    "    },\n",
    "    # QuadraticDiscriminantAnalysis\n",
    "    {\n",
    "        'reg_param': [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    },\n",
    "    # RandomForestClassifier\n",
    "    {\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'n_estimators': [100, 200, 300, 400, 500]\n",
    "    },\n",
    "    # SVC\n",
    "    {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': np.arange(1, 3, 1),\n",
    "        'gamma': np.arange(0.25, 1, 0.25)\n",
    "    },\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = []\n",
    "for dataset in DATASETS:\n",
    "    file = os.path.join(PROCESSED, dataset)\n",
    "    if file.split('.')[-1] == 'parquet':\n",
    "        df = pd.read_parquet(file)\n",
    "        dff.append(df)\n",
    "\n",
    "    df = pd.concat(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axs = axs.ravel()\n",
    "\n",
    "df['id1'].value_counts().sort_index().plot(\n",
    "    kind='bar', title='ID1 Distribution', xlabel='IDs', ylabel='Occurences', ax=axs[0])\n",
    "df['id2'].value_counts().sort_index().plot(\n",
    "    kind='bar', title='ID2 Distribution', xlabel='IDs', ylabel='Occurences', ax=axs[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "saveDistPath = os.path.join(imageFolder, saveBase)\n",
    "saveDistPath += '_unbalancedDistribution.pdf'\n",
    "plt.savefig(saveDistPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if customBalance:\n",
    "    id1_dict = {}\n",
    "    id2_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if targetedUndersampling:\n",
    "    df_x = df.drop(id, axis=1)\n",
    "    df_x = df_x.drop(ids_remove, axis=1)\n",
    "    if customBalance:\n",
    "        X_resampled, y_resampled = RandomUnderSampler(\n",
    "            sampling_strategy=id2_dict, random_state=SEED).fit_resample(df_x, df[id])\n",
    "    else:\n",
    "        X_resampled, y_resampled = RandomUnderSampler(random_state=SEED).fit_resample(df_x, df[id])\n",
    "\n",
    "    X_resampled[id] = y_resampled\n",
    "    # for id_remove in ids_remove:\n",
    "    #     X_resampled[id_remove]\n",
    "    df = X_resampled\n",
    "\n",
    "    df[id].value_counts().sort_index().plot(\n",
    "        kind='bar', title='ID1 Distribution', xlabel='IDs', ylabel='Occurences')\n",
    "    \n",
    "    saveBalPath = os.path.join(imageFolder, saveBase)\n",
    "    saveBalPath += '_balancedDistribution.pdf'\n",
    "    plt.savefig(saveBalPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beforeFeat = df.shape[1]\n",
    "tsfresh.utilities.dataframe_functions.impute(df)\n",
    "\n",
    "if filterFeatures:\n",
    "    df = tsfresh.select_features(df, df[id])\n",
    "    afterFeat = df.shape[1]\n",
    "\n",
    "    print(f'[üî• FILTER]\\n\\tBefore: {beforeFeat}\\n\\tAfter: {afterFeat}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading labels\n",
    "labels = df[id][:, np.newaxis]\n",
    "\n",
    "if authentication:\n",
    "    # Lists of datasets\n",
    "    X_trains = []\n",
    "    X_tests = []\n",
    "    Y_trains = []\n",
    "    Y_tests = []\n",
    "    all_balances = []\n",
    "\n",
    "    for balance in BALANCES:\n",
    "        # Translating to authentication, i.e., taking only one label\n",
    "        # Saving different dataset, one for each label\n",
    "        for label in np.unique(labels):\n",
    "            labels_auth = []\n",
    "            for l in labels:\n",
    "                if l == label:\n",
    "                    labels_auth.append(1)\n",
    "                else:\n",
    "                    labels_auth.append(0)\n",
    "\n",
    "            labels_auth = np.array(labels_auth)\n",
    "\n",
    "            l0 = 0\n",
    "            l1 = 0\n",
    "            for l in labels_auth:\n",
    "                if l == 0:\n",
    "                    l0 += 1\n",
    "                else:\n",
    "                    l1 += 1\n",
    "\n",
    "            # How many 0s would I need by looking at the 1 class\n",
    "            requested_0 = int(l1 / balance[1] * balance[0])\n",
    "            # How many 1s would I need by looking at the 0 class\n",
    "            requested_1 = int(l0 / balance[0] * balance[1])\n",
    "\n",
    "            if requested_0 < l0:\n",
    "                num_samples = {\n",
    "                    0: requested_0,\n",
    "                    1: l1\n",
    "                }\n",
    "            if requested_1 < l1:\n",
    "                num_samples = {\n",
    "                    0: l0,\n",
    "                    1: requested_1\n",
    "                }\n",
    "\n",
    "            # Loading features\n",
    "            features = df.drop(id, axis=1)\n",
    "\n",
    "            # Dataset balancing\n",
    "            features, labels_auth = RandomUnderSampler(sampling_strategy=num_samples,\n",
    "                random_state=SEED).fit_resample(features, labels_auth)\n",
    "\n",
    "            # Train and test split\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                features, labels_auth, test_size=0.2, random_state=SEED)\n",
    "\n",
    "            cols = []\n",
    "            for col in X_train.columns:\n",
    "                cols.append(col.replace('z2__', ''))\n",
    "\n",
    "            X_train.columns = cols\n",
    "            X_test.columns = cols\n",
    "\n",
    "            X_trains.append(X_train)\n",
    "            X_tests.append(X_test)\n",
    "            Y_trains.append(Y_train)\n",
    "            Y_tests.append(Y_test)\n",
    "            all_balances.append(balance)\n",
    "else:\n",
    "    # Loading features\n",
    "    features = df.drop(id, axis=1)\n",
    "    if not targetedUndersampling:\n",
    "        for id_remove in ids_remove:\n",
    "            features = features.drop(id_remove, axis=1)\n",
    "\n",
    "    # Train and test split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=SEED)\n",
    "\n",
    "    cols = []\n",
    "    for col in X_train.columns:\n",
    "        cols.append(col.replace('z2__', ''))\n",
    "\n",
    "    X_train.columns = cols\n",
    "    X_test.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "best_params = []\n",
    "model_balances = []\n",
    "\n",
    "fars = []\n",
    "frrs = []\n",
    "\n",
    "complexity = []\n",
    "\n",
    "# Iterate over classifiers\n",
    "for name, clf, param in zip(names, classifiers, parameters):\n",
    "    if authentication:\n",
    "        score_trains = []\n",
    "        accuracy_tests = []\n",
    "        precision_tests = []\n",
    "        recall_tests = []\n",
    "        f1_tests = []\n",
    "        fars_tests = []\n",
    "        frrs_tests = []\n",
    "        for i, (X_train, X_test, Y_train, Y_test, balance) in enumerate(zip(X_trains, X_tests, Y_trains, Y_tests, all_balances)):\n",
    "            # Defining GridSearch\n",
    "            grid = GridSearchCV(clf, param, n_jobs=-1, verbose=0)\n",
    "            print(f'[ü§ñ MODEL] {name} ({i+1}/{len(X_trains)})', end='\\r')\n",
    "            # Fitting the model\n",
    "            grid.fit(X_train, Y_train)\n",
    "            # Training score\n",
    "            score_trains.append(grid.best_estimator_.score(X_train, Y_train))\n",
    "            # Test scores\n",
    "            start_pred = time.time()\n",
    "            Y_pred = grid.best_estimator_.predict(X_test)\n",
    "            end_pred = time.time()\n",
    "\n",
    "            pred_time = end_pred - start_pred\n",
    "            size = sys.getsizeof(pickle.dumps(grid.best_estimator_))\n",
    "\n",
    "            complexity.append({\n",
    "                'Model': name,\n",
    "                'Prediction_Time': pred_time,\n",
    "                'Size': size\n",
    "            })\n",
    "            \n",
    "            accuracy_tests.append(accuracy_score(Y_test, Y_pred))\n",
    "            precision_tests.append(precision_score(Y_test, Y_pred))\n",
    "            recall_tests.append(recall_score(Y_test, Y_pred))\n",
    "            f1_tests.append(f1_score(Y_test, Y_pred))\n",
    "            # FARs and FRRs\n",
    "            cm = confusion_matrix(y_true=Y_test, y_pred=Y_pred)\n",
    "            tp = cm[0][0]\n",
    "            tn = cm[1][1]\n",
    "            fp = cm[0][1]\n",
    "            fn = cm[1][0]\n",
    "            far = fp/(fp+tn)\n",
    "            if math.isnan(far):\n",
    "                far = 0\n",
    "            frr = fn/(fn+tp)\n",
    "            if math.isnan(frr):\n",
    "                frr = 0\n",
    "            fars_tests.append(far)\n",
    "            frrs_tests.append(frr)\n",
    "            \n",
    "            # Save results by doing the mean on each balance value\n",
    "            if (i+1) % len(np.unique(labels)) == 0:\n",
    "                print()\n",
    "                print(f'\\t[‚öñÔ∏è BALANCE] {balance[0]}/{balance[1]}')\n",
    "                print(f'\\t\\t[üí™ TRAIN]\\t{round(np.mean(score_trains), 3)}')\n",
    "                print(f'\\t\\t[üìä ACCURACY]\\t{round(np.mean(accuracy_tests), 3)}')\n",
    "                print(f'\\t\\t[üìä PRECISION]\\t{round(np.mean(precision_tests), 3)}')\n",
    "                print(f'\\t\\t[üìä RECALL]\\t{round(np.mean(recall_tests), 3)}')\n",
    "                print(f'\\t\\t[üìä F1 SCORE]\\t{round(np.mean(f1_tests), 3)}')\n",
    "                print(f'\\t\\t[üîê FAR]\\t{round(np.mean(fars_tests), 3)}')\n",
    "                print(f'\\t\\t[üîê FRR]\\t{round(np.mean(frrs_tests), 3)}\\n')\n",
    "                # Saving results\n",
    "                train_scores.append(round(np.mean(score_trains), 3))\n",
    "                accuracy_scores.append(round(np.mean(accuracy_tests), 3))\n",
    "                precision_scores.append(round(np.mean(precision_tests), 3))\n",
    "                recall_scores.append(round(np.mean(recall_tests), 3))\n",
    "                f1_scores.append(round(np.mean(f1_tests), 3))\n",
    "                fars.append(round(np.mean(fars_tests), 3))\n",
    "                frrs.append(round(np.mean(frrs_tests), 3))\n",
    "                best_params.append(grid.best_params_)\n",
    "                model_balances.append(balance)\n",
    "                # Resetting results\n",
    "                score_trains = []\n",
    "                accuracy_tests = []\n",
    "                precision_tests = []\n",
    "                recall_tests = []\n",
    "                f1_tests = []\n",
    "                fars_tests = []\n",
    "                frrs_tests = []\n",
    "\n",
    "        print('-' * 50)\n",
    "        print()\n",
    "    else:\n",
    "        print(f'[ü§ñ MODEL] {name}')\n",
    "        # Defining GridSearch\n",
    "        grid = GridSearchCV(clf, param, n_jobs=-1, verbose=0)\n",
    "        # Fitting the model\n",
    "        grid.fit(X_train, Y_train)\n",
    "        # Training score\n",
    "        score_train = grid.best_estimator_.score(X_train, Y_train)\n",
    "        print(f'\\t[üí™ TRAIN]\\t{round(score_train, 3)}')\n",
    "        # Test scores\n",
    "        # Test scores\n",
    "        start_pred = time.time()\n",
    "        Y_pred = grid.best_estimator_.predict(X_test)\n",
    "        end_pred = time.time()\n",
    "\n",
    "        pred_time = end_pred - start_pred\n",
    "        size = sys.getsizeof(pickle.dumps(grid.best_estimator_))\n",
    "\n",
    "        complexity.append({\n",
    "            'Model': name,\n",
    "            'Prediction_Time': pred_time,\n",
    "            'Size': size\n",
    "        })\n",
    "        accuracy = accuracy_score(Y_test, Y_pred)\n",
    "        precision = precision_score(Y_test, Y_pred, average='macro')\n",
    "        recall = recall_score(Y_test, Y_pred, average='macro')\n",
    "        f1 = f1_score(Y_test, Y_pred, average='macro')\n",
    "\n",
    "        print(f'\\t[üìä ACCURACY]\\t{round(accuracy, 3)}')\n",
    "        print(f'\\t[üìä PRECISION]\\t{round(precision, 3)}')\n",
    "        print(f'\\t[üìä RECALL]\\t{round(recall, 3)}')\n",
    "        print(f'\\t[üìä F1 SCORE]\\t{round(f1, 3)}\\n')\n",
    "\n",
    "        train_scores.append(round(score_train, 3))\n",
    "        accuracy_scores.append(round(accuracy, 3))\n",
    "        precision_scores.append(round(precision, 3))\n",
    "        recall_scores.append(round(recall, 3))\n",
    "        f1_scores.append(round(f1, 3))\n",
    "        best_params.append(grid.best_params_)\n",
    "\n",
    "    # Feature importance for Random Forest\n",
    "    if name == 'Random Forest':\n",
    "        # Confusion Matrix\n",
    "        conf_matrix = confusion_matrix(y_true=Y_test, y_pred=Y_pred)\n",
    "        # Explainable ML\n",
    "        impurity = grid.best_estimator_.feature_importances_\n",
    "        std = np.std([tree.feature_importances_ for tree in grid.best_estimator_.estimators_], axis=0)\n",
    "        explainer = shap.TreeExplainer(grid.best_estimator_)\n",
    "        shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i, s=conf_matrix[i, j],\n",
    "                va='center', ha='center', size='xx-large')\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix for Random Forest', fontsize=18)\n",
    "\n",
    "saveConfPath = os.path.join(imageFolder, saveBase)\n",
    "saveConfPath += '_confusionMatrix.pdf'\n",
    "plt.savefig(saveConfPath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîù Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_impurity = pd.Series(impurity, index=X_train.columns).nlargest(20)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_impurity.plot.bar(ax=ax)  # , yerr=std)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "# plt.xticks(rotation = 90)\n",
    "fig.tight_layout()\n",
    "\n",
    "saveMDIPath = os.path.join(imageFolder, saveBase)\n",
    "saveMDIPath += '_MDI.pdf'\n",
    "plt.savefig(saveMDIPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", show=False)\n",
    "saveShapPath = os.path.join(imageFolder, saveBase)\n",
    "saveShapPath += '_Shap.pdf'\n",
    "plt.savefig(saveShapPath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "name_idx = 0\n",
    "for i, (acc, prec, rec, f1) in enumerate(zip(accuracy_scores, precision_scores, recall_scores, f1_scores)):\n",
    "    df_list.append({\n",
    "        'Model': names[name_idx],\n",
    "        'ID': id[-1],\n",
    "        'Authentication': authentication,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1': f1\n",
    "    })\n",
    "    if authentication:\n",
    "        if (i+1) % len(BALANCES) == 0:\n",
    "            name_idx += 1\n",
    "    else:\n",
    "        name_idx += 1\n",
    "\n",
    "if authentication:\n",
    "    for i, (far, frr, dfl) in enumerate(zip(fars, frrs, df_list)):\n",
    "        dfl['FAR'] = far\n",
    "        dfl['FRR'] = frr\n",
    "        dfl['Balance'] = str(model_balances[i][0]) + '/' + str(model_balances[i][1])\n",
    "\n",
    "df_csv = pd.DataFrame(df_list)\n",
    "savePath = os.path.join(RESULTS, saveBase)\n",
    "savePath += '.csv'\n",
    "df_csv.to_csv(savePath)\n",
    "\n",
    "# Saving times and sizes\n",
    "df_comp = pd.DataFrame(complexity)\n",
    "timeSavePath = os.path.join(COMPLEXITY, saveBase)\n",
    "timeSavePath += '.csv'\n",
    "df_comp.to_csv(timeSavePath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
