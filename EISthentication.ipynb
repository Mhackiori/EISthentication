{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîê Authentication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shap\n",
    "import sys\n",
    "import tsfresh\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from utils.const import *\n",
    "from utils.helperFunctions import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"  # Also affect subprocesses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìç Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing IDs\n",
    "ids = ['id1', 'id2', 'id3']\n",
    "# Choose what ID to process\n",
    "id = 'id1'\n",
    "ids_remove = [x for x in ids if x != id]\n",
    "\n",
    "# Filter features and keep only relevant ones\n",
    "filterFeatures = True\n",
    "\n",
    "# Undersampling\n",
    "fairUndersampling = False       # Each class same number\n",
    "targetedUndersampling = True    # Downsample most frequent class\n",
    "customBalance = False           # Downsample by specifying number of samples for each label\n",
    "\n",
    "# If True, perform authentication. If False, perform identification\n",
    "# - Authentication: binary classification, unbalanced\n",
    "# - Identificatiom: multiclass classification, balanced\n",
    "authentication = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    'AdaBoost',\n",
    "    'Decision Tree',\n",
    "    'Gaussian Naive Bayes',\n",
    "    'Nearest Neighbors',\n",
    "    'Neural Network',\n",
    "    'Quadratic Discriminant Analysis',\n",
    "    'Random Forest',\n",
    "    'Support Vector Machine'\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    AdaBoostClassifier(random_state=SEED),\n",
    "    DecisionTreeClassifier(random_state=SEED),\n",
    "    GaussianNB(),\n",
    "    KNeighborsClassifier(),\n",
    "    MLPClassifier(random_state=SEED),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    RandomForestClassifier(random_state=SEED),\n",
    "    SVC(random_state=SEED),\n",
    "]\n",
    "\n",
    "parameters = [\n",
    "    # AdaBoostClassifier\n",
    "    {\n",
    "        'n_estimators': [50, 100, 150, 200]\n",
    "    },\n",
    "    # DecisionTreeClassifier\n",
    "    {\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_depth': np.arange(3, 20)\n",
    "    },\n",
    "    # GaussianNB\n",
    "    {\n",
    "        'var_smoothing': np.logspace(0, -9, num=100)\n",
    "    },\n",
    "    # KNeighborsClassifier\n",
    "    {\n",
    "        'n_neighbors': list(range(1, 20)),\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    # MLPClassifier\n",
    "    {\n",
    "        'hidden_layer_sizes': [(50, ), (100, ), (200, )],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['adam', 'sgd']\n",
    "    },\n",
    "    # QuadraticDiscriminantAnalysis\n",
    "    {\n",
    "        'reg_param': [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    },\n",
    "    # RandomForestClassifier\n",
    "    {\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'n_estimators': [100, 200, 300, 400, 500]\n",
    "    },\n",
    "    # SVC\n",
    "    {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': np.arange(1, 5, 1),\n",
    "        'gamma': np.arange(0.2, 1, 0.2)\n",
    "    },\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = []\n",
    "for dataset in DATASETS:\n",
    "    file = os.path.join(PROCESSED, dataset)\n",
    "    if file.split('.')[-1] == 'parquet':\n",
    "        df = pd.read_parquet(file)\n",
    "        dff.append(df)\n",
    "\n",
    "    df = pd.concat(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axs = axs.ravel()\n",
    "\n",
    "df['id1'].value_counts().sort_index().plot(\n",
    "    kind='bar', title='ID1 Distribution', xlabel='IDs', ylabel='Occurences', ax=axs[0])\n",
    "df['id2'].value_counts().sort_index().plot(\n",
    "    kind='bar', title='ID2 Distribution', xlabel='IDs', ylabel='Occurences', ax=axs[1])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if customBalance:\n",
    "    id1_dict = {}\n",
    "    id2_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if targetedUndersampling:\n",
    "    df_x = df.drop(id, axis=1)\n",
    "    df_x = df_x.drop(ids_remove, axis=1)\n",
    "    if customBalance:\n",
    "        X_resampled, y_resampled = RandomUnderSampler(\n",
    "            sampling_strategy=id2_dict, random_state=SEED).fit_resample(df_x, df[id])\n",
    "    else:\n",
    "        X_resampled, y_resampled = RandomUnderSampler(random_state=SEED).fit_resample(df_x, df[id])\n",
    "\n",
    "    X_resampled[id] = y_resampled\n",
    "    # for id_remove in ids_remove:\n",
    "    #     X_resampled[id_remove]\n",
    "    df = X_resampled\n",
    "\n",
    "    df[id].value_counts().sort_index().plot(\n",
    "        kind='bar', title='ID1 Distribution', xlabel='IDs', ylabel='Occurences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beforeFeat = df.shape[1]\n",
    "tsfresh.utilities.dataframe_functions.impute(df)\n",
    "\n",
    "if filterFeatures:\n",
    "    df = tsfresh.select_features(df, df[id])\n",
    "    afterFeat = df.shape[1]\n",
    "\n",
    "    print(f'[üî• FILTER]\\n\\tBefore: {beforeFeat}\\n\\tAfter: {afterFeat}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading labels\n",
    "labels = df[id][:, np.newaxis]\n",
    "\n",
    "if authentication:\n",
    "    # Lists of datasets\n",
    "    X_trains = []\n",
    "    X_tests = []\n",
    "    Y_trains = []\n",
    "    Y_tests = []\n",
    "\n",
    "    # Translating to authentication, i.e., taking only one label\n",
    "    # Saving different dataset, one for each label\n",
    "    for label in np.unique(labels):\n",
    "        labels_auth = []\n",
    "        for l in labels:\n",
    "            if l == label:\n",
    "                labels_auth.append(1)\n",
    "            else:\n",
    "                labels_auth.append(0)\n",
    "\n",
    "        labels_auth = np.array(labels_auth)\n",
    "\n",
    "        # Loading features\n",
    "        features = df.drop(id, axis=1)\n",
    "\n",
    "        # Train and test split\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            features, labels_auth, test_size=0.2, random_state=SEED)\n",
    "\n",
    "        cols = []\n",
    "        for col in X_train.columns:\n",
    "            cols.append(col.replace('z2__', ''))\n",
    "\n",
    "        X_train.columns = cols\n",
    "        X_test.columns = cols\n",
    "\n",
    "        X_trains.append(X_train)\n",
    "        X_tests.append(X_test)\n",
    "        Y_trains.append(Y_train)\n",
    "        Y_tests.append(Y_test)\n",
    "else:\n",
    "    # Loading features\n",
    "    features = df.drop(id, axis=1)\n",
    "    if not targetedUndersampling:\n",
    "        for id_remove in ids_remove:\n",
    "            features = features.drop(id_remove, axis=1)\n",
    "\n",
    "    # Train and test split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=SEED)\n",
    "\n",
    "    cols = []\n",
    "    for col in X_train.columns:\n",
    "        cols.append(col.replace('z2__', ''))\n",
    "\n",
    "    X_train.columns = cols\n",
    "    X_test.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "best_params = []\n",
    "\n",
    "# Iterate over classifiers\n",
    "for name, clf, param in zip(names, classifiers, parameters):\n",
    "    if authentication:\n",
    "        score_trains = []\n",
    "        accuracy_tests = []\n",
    "        precision_tests = []\n",
    "        recall_tests = []\n",
    "        f1_tests = []\n",
    "        for i, (X_train, X_test, Y_train, Y_test) in enumerate(zip(X_trains, X_tests, Y_trains, Y_tests)):\n",
    "            # Defining GridSearch\n",
    "            grid = GridSearchCV(clf, param, n_jobs=-1, verbose=0)\n",
    "            print(f'[ü§ñ MODEL] {name} ({i+1}/{len(X_trains)})', end='\\r')\n",
    "            # Fitting the model\n",
    "            grid.fit(X_train, Y_train)\n",
    "            # Training score\n",
    "            score_trains.append(grid.best_estimator_.score(X_train, Y_train))\n",
    "            # Test scores\n",
    "            Y_pred = grid.best_estimator_.predict(X_test)\n",
    "            accuracy_tests.append(accuracy_score(Y_test, Y_pred))\n",
    "            precision_tests.append(precision_score(Y_test, Y_pred))\n",
    "            recall_tests.append(recall_score(Y_test, Y_pred))\n",
    "            f1_tests.append(f1_score(Y_test, Y_pred))\n",
    "            \n",
    "        print()\n",
    "        print(f'\\t[üí™ TRAIN]\\t{round(np.mean(score_trains), 3)}')\n",
    "        print(f'\\t[üìä ACCURACY]\\t{round(np.mean(accuracy_tests), 3)}')\n",
    "        print(f'\\t[üìä PRECISION]\\t{round(np.mean(precision_tests), 3)}')\n",
    "        print(f'\\t[üìä RECALL]\\t{round(np.mean(recall_tests), 3)}')\n",
    "        print(f'\\t[üìä F1 SCORE]\\t{round(np.mean(f1_tests), 3)}\\n')\n",
    "        best_params.append(grid.best_params_)\n",
    "    else:\n",
    "        print(f'[ü§ñ MODEL] {name}')\n",
    "        # Defining GridSearch\n",
    "        grid = GridSearchCV(clf, param, n_jobs=-1, verbose=0)\n",
    "        # Fitting the model\n",
    "        grid.fit(X_train, Y_train)\n",
    "        # Training score\n",
    "        score_train = grid.best_estimator_.score(X_train, Y_train)\n",
    "        print(f'\\t[üí™ TRAIN]\\t{round(score_train, 3)}')\n",
    "        # Test scores\n",
    "        Y_pred = grid.best_estimator_.predict(X_test)\n",
    "        accuracy = accuracy_score(Y_test, Y_pred)\n",
    "        precision = precision_score(Y_test, Y_pred, average='macro')\n",
    "        recall = recall_score(Y_test, Y_pred, average='macro')\n",
    "        f1 = f1_score(Y_test, Y_pred, average='macro')\n",
    "\n",
    "        print(f'\\t[üìä ACCURACY]\\t{round(accuracy, 3)}')\n",
    "        print(f'\\t[üìä PRECISION]\\t{round(precision, 3)}')\n",
    "        print(f'\\t[üìä RECALL]\\t{round(recall, 3)}')\n",
    "        print(f'\\t[üìä F1 SCORE]\\t{round(f1, 3)}\\n')\n",
    "\n",
    "        train_scores.append(score_train)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        best_params.append(grid.best_params_)\n",
    "\n",
    "    # Feature importance for Random Forest\n",
    "    if name == 'Random Forest':\n",
    "        # Confusion Matrix\n",
    "        conf_matrix = confusion_matrix(y_true=Y_test, y_pred=Y_pred)\n",
    "        # Explainable ML\n",
    "        impurity = grid.best_estimator_.feature_importances_\n",
    "        std = np.std([tree.feature_importances_ for tree in grid.best_estimator_.estimators_], axis=0)\n",
    "        explainer = shap.TreeExplainer(grid.best_estimator_)\n",
    "        shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i, s=conf_matrix[i, j],\n",
    "                va='center', ha='center', size='xx-large')\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix for Random Forest', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîù Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_impurity = pd.Series(impurity, index=X_train.columns).nlargest(20)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_impurity.plot.bar(ax=ax)  # , yerr=std)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "# plt.xticks(rotation = 90)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
